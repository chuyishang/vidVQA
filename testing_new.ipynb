{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features shape:  torch.Size([11, 1152]) Text features shape:  torch.Size([1, 1152])\n",
      "[('apple-ipod.jpg', 0.0), ('apple-blank.jpg', 0.0), ('cold_drink.jpg', 0.0), ('hot_drink.jpg', 0.0), ('caffeine.jpg', 0.0), ('siglip.jpg', 0.0), ('authors.jpg', 0.0), ('robosign.jpg', 0.0), ('cow_beach.jpg', 0.001), ('cow_beach2.jpg', 0.99), ('mountain_view.jpg', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, preprocess = create_model_from_pretrained('hf-hub:timm/ViT-SO400M-14-SigLIP-384')\n",
    "model = model.to(device)\n",
    "preprocess = preprocess\n",
    "tokenizer = get_tokenizer('hf-hub:timm/ViT-SO400M-14-SigLIP-384')\n",
    "\n",
    "filenames = [\n",
    "    'robosign.jpg',\n",
    "    'cow_beach.jpg',\n",
    "    'cow_beach2.jpg',\n",
    "    'mountain_view.jpg',]\n",
    "\n",
    "\"\"\" 'apple-ipod.jpg',\n",
    "    'apple-blank.jpg',\n",
    "    'cold_drink.jpg',\n",
    "    'hot_drink.jpg',\n",
    "    'caffeine.jpg',\n",
    "    'siglip.jpg',\n",
    "    'authors.jpg',\"\"\"\n",
    "\n",
    "\n",
    "#@title Load and embed images\n",
    "images = [Image.open(\"./data/\" + fname) for fname in (filenames)]\n",
    "\n",
    "labels_list = [\"a cow in a tuxedo\"]\n",
    "image_stack = torch.stack([preprocess(image) for image in images]).to(device)\n",
    "text = tokenizer(labels_list, context_length=model.context_length).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image_stack)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features = F.normalize(image_features, dim=-1)\n",
    "    text_features = F.normalize(text_features, dim=-1)\n",
    "    print(\"Image features shape: \", image_features.shape, \"Text features shape: \", text_features.shape)\n",
    "\n",
    "    text_probs = torch.sigmoid(text_features @ image_features.T * model.logit_scale.exp() + model.logit_bias)\n",
    "\n",
    "zipped_list = list(zip(filenames, [round(p.item(), 3) for p in text_probs[0]]))\n",
    "print(zipped_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vid_vqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
