{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n",
    "import decord\n",
    "from decord import cpu, gpu\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "import ast\n",
    "from lavis.models import load_model_and_preprocess\n",
    "import openai\n",
    "import utils\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda cuda\n"
     ]
    }
   ],
   "source": [
    "with open('api.key') as f:\n",
    "    openai.api_key = f.read().strip()\n",
    "with open('api_org.key') as f:\n",
    "    openai.organization = f.read().strip()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device2 = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device, device2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video(video_path, fps=30):\n",
    "        # If fixed width and height are required, VideoReader takes width and height as arguments.\n",
    "        video_reader = decord.VideoReader(str(video_path), num_threads=1, ctx=cpu(0))\n",
    "        decord.bridge.set_bridge('torch')\n",
    "        vlen = len(video_reader)\n",
    "        #print(vlen)\n",
    "        original_fps = video_reader.get_avg_fps()\n",
    "        #print(original_fps)\n",
    "        num_frames = int(vlen * fps / original_fps)\n",
    "        # num_frames = min(self.max_num_frames, num_frames)\n",
    "        frame_idxs = np.linspace(0, vlen, num_frames, endpoint=False).astype(np.int_)\n",
    "        video = video_reader.get_batch(frame_idxs).byte()\n",
    "        video = video.permute(0, 3, 1, 2)\n",
    "        return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt):\n",
    "    completion = openai.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Follow the directions given in the next prompt carefully.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    )\n",
    "    output_message = completion.choices[0].message.content\n",
    "    return output_message\n",
    "\n",
    "def get_answer_helper(question, answer_choices, curr_frame, total_frames, caption, prev_info=None):\n",
    "    with open('./prompts/base_prompt.txt') as f:\n",
    "        prompt = f.read()\n",
    "    prompt = prompt.replace('insert_question', question)\n",
    "    prompt = prompt.replace('insert_choices', str(answer_choices))\n",
    "    prompt = prompt.replace('insert_curr_frame', str(curr_frame))\n",
    "    prompt = prompt.replace('insert_total_frames', str(total_frames))\n",
    "    prompt = prompt.replace('insert_caption', caption[0])\n",
    "\n",
    "    #print(prompt)\n",
    "    output = call_llm(prompt)\n",
    "    try:\n",
    "        output_dict = ast.literal_eval(output)\n",
    "        print(\"GETTING OUTPUT: \", output_dict)\n",
    "        return output_dict\n",
    "    except:\n",
    "        print(\"ERROR: \", output)\n",
    "\n",
    "def final_select(question, choices, info):\n",
    "    with open('./prompts/final_prompt.txt') as f:\n",
    "        prompt = f.read()\n",
    "    prompt = prompt.replace('insert_question', question)\n",
    "    prompt = prompt.replace('insert_choices', str(choices))\n",
    "    prompt = prompt.replace('insert_info', str(info))\n",
    "    #print(prompt)\n",
    "    output = call_llm(prompt)\n",
    "    try:\n",
    "        output_dict = ast.literal_eval(output)\n",
    "        print(\"GETTING FINAL OUTPUT: \", output_dict)\n",
    "        return output_dict\n",
    "    except:\n",
    "        print(\"ERROR: \", output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_VQA(vqa_question, frame):\n",
    "    print(\"CALLING VQA: \", vqa_question)\n",
    "    model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip_vqa\", model_type=\"vqav2\", is_eval=True, device=device2)\n",
    "    # ask a random question.\n",
    "    question = vqa_question\n",
    "    image = vis_processors[\"eval\"](frame).unsqueeze(0).to(device2)\n",
    "    question = txt_processors[\"eval\"](question)\n",
    "    answer = model.predict_answers(samples={\"image\": image, \"text_input\": question}, inference_method=\"generate\")\n",
    "    print(\"VQA ANSWER: \", answer)\n",
    "    return answer\n",
    "\n",
    "def query_caption(frame):\n",
    "    model, vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device2)\n",
    "    image = vis_processors[\"eval\"](frame).unsqueeze(0).to(device2)\n",
    "    caption = model.generate({\"image\": image})\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(images, question, answer_choices, curr_frame, total_frames, caption, prev_info=None):\n",
    "    LIMIT = 10\n",
    "    goto_frame = curr_frame\n",
    "    VQA_question = None\n",
    "    info = {}\n",
    "    caption = caption\n",
    "    while LIMIT >= 0:\n",
    "        print(f\"CALL {10 - LIMIT}\")\n",
    "        print(\"ALL INFO:\", info)\n",
    "        if f\"Frame {goto_frame}\" not in info:\n",
    "            info[f\"Frame {goto_frame}\"] = {}\n",
    "        LIMIT -= 1  \n",
    "        if goto_frame != None:\n",
    "            raw_image = images[goto_frame] \n",
    "        caption = query_caption(raw_image)\n",
    "        print(caption)\n",
    "        output = get_answer_helper(question, answer_choices, goto_frame, total_frames, caption, prev_info)\n",
    "        print(output)\n",
    "        if output[\"Answer\"] != None:\n",
    "            print(\"here1\")\n",
    "            return output[\"Answer\"]\n",
    "        else:\n",
    "            info[f\"Frame {goto_frame}\"][\"caption\"] = caption\n",
    "            if output[\"Choose Frame\"] != None:\n",
    "                goto_frame = output[\"Choose Frame\"]\n",
    "            if output[\"VQA Question\"] != None:\n",
    "                VQA_question = output[\"VQA Question\"]\n",
    "                vqa_result = query_VQA(VQA_question, raw_image)\n",
    "                info[f\"Frame {curr_frame}\"][VQA_question] = vqa_result\n",
    "            else:\n",
    "                continue       \n",
    "    # case for when we run out of tries\n",
    "    final_output = final_select(question, answer_choices, info)\n",
    "    return final_output[\"Answer\"]\n",
    "\n",
    "def answer_question_half(vid_id, quest, option_choices, query):\n",
    "    video_id = vid_id\n",
    "    question = quest\n",
    "    options = option_choices\n",
    "    video = get_video(f'/shared/shang/datasets/nextqa/videos/{video_id}.mp4')\n",
    "\n",
    "    model, preprocess = create_model_from_pretrained('hf-hub:timm/ViT-SO400M-14-SigLIP-384')\n",
    "    model = model.to(device)\n",
    "    preprocess = preprocess\n",
    "    tokenizer = get_tokenizer('hf-hub:timm/ViT-SO400M-14-SigLIP-384')\n",
    "\n",
    "    transform = T.ToPILImage()\n",
    "    sampling_rate = 10\n",
    "    images = [transform(video[i]) for i in range(0, video.shape[0], sampling_rate)]\n",
    "\n",
    "    labels_list = query\n",
    "    image_stack = torch.stack([preprocess(image) for image in images]).to(device)\n",
    "    text = tokenizer(labels_list, context_length=model.context_length).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        image_features = model.encode_image(image_stack)\n",
    "        text_features = model.encode_text(text)\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        #print(\"Image features shape: \", image_features.shape, \"Text features shape: \", text_features.shape)\n",
    "\n",
    "        text_probs = torch.sigmoid(text_features @ image_features.T * model.logit_scale.exp() + model.logit_bias)\n",
    "\n",
    "    values, indices = torch.topk(text_probs, 3)\n",
    "\n",
    "    raw_image = images[indices[0][0].item()]\n",
    "    print(f\"{indices[0][0].item()}/{len(images)}\")\n",
    "\n",
    "    from lavis.models import load_model_and_preprocess\n",
    "    model, vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device2)\n",
    "    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device2)\n",
    "    caption = model.generate({\"image\": image})\n",
    "    print(caption)\n",
    "    raw_image.show()\n",
    "    return images, caption, indices[0][0].item(), len(images) - 1\n",
    "    #result = get_answer(images, question, options, indices[0][0].item(), len(images), caption, None)\n",
    "    #return result\n",
    "\n",
    "def answer_question(vid_id, quest, option_choices, query):\n",
    "    video_id = vid_id\n",
    "    question = quest\n",
    "    options = option_choices\n",
    "    video = get_video(f'/shared/shang/datasets/nextqa/videos/{video_id}.mp4')\n",
    "\n",
    "    model, preprocess = create_model_from_pretrained('hf-hub:timm/ViT-SO400M-14-SigLIP-384')\n",
    "    model = model.to(device)\n",
    "    preprocess = preprocess\n",
    "    tokenizer = get_tokenizer('hf-hub:timm/ViT-SO400M-14-SigLIP-384')\n",
    "\n",
    "    transform = T.ToPILImage()\n",
    "    sampling_rate = 10\n",
    "    images = [transform(video[i]) for i in range(0, video.shape[0], sampling_rate)]\n",
    "\n",
    "    labels_list = query\n",
    "    image_stack = torch.stack([preprocess(image) for image in images]).to(device)\n",
    "    text = tokenizer(labels_list, context_length=model.context_length).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        image_features = model.encode_image(image_stack)\n",
    "        text_features = model.encode_text(text)\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        #print(\"Image features shape: \", image_features.shape, \"Text features shape: \", text_features.shape)\n",
    "\n",
    "        text_probs = torch.sigmoid(text_features @ image_features.T * model.logit_scale.exp() + model.logit_bias)\n",
    "\n",
    "    values, indices = torch.topk(text_probs, 3)\n",
    "\n",
    "    raw_image = images[indices[0][0].item()]\n",
    "    print(f\"{indices[0][0].item()}/{len(images)}\")\n",
    "\n",
    "    from lavis.models import load_model_and_preprocess\n",
    "    model, vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device2)\n",
    "    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device2)\n",
    "    caption = model.generate({\"image\": image})\n",
    "    print(caption)\n",
    "    raw_image.show()\n",
    "    result = get_answer(images, question, options, indices[0][0].item(), len(images)-1, caption, None)\n",
    "    return result, vid_id, question, options, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siglip_query(question):\n",
    "    with open('./prompts/base_query.txt') as f:\n",
    "        prompt = f.read()\n",
    "        prompt = prompt.replace('insert_question', question)\n",
    "        output = utils.call_llm(prompt)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def get_frame(query, video_id): \n",
    "    video = get_video(f'/shared/shang/datasets/nextqa/videos/{video_id}.mp4')\n",
    "    model, preprocess = create_model_from_pretrained('hf-hub:timm/ViT-SO400M-14-SigLIP-384')\n",
    "    model = model.to(device)\n",
    "    preprocess = preprocess\n",
    "    tokenizer = get_tokenizer('hf-hub:timm/ViT-SO400M-14-SigLIP-384')\n",
    "    transform = T.ToPILImage()\n",
    "    sampling_rate = 10\n",
    "    images = [transform(video[i]) for i in range(0, video.shape[0], sampling_rate)]\n",
    "\n",
    "    labels_list = query\n",
    "    image_stack = torch.stack([preprocess(image) for image in images]).to(device)\n",
    "    text = tokenizer(labels_list, context_length=model.context_length).to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        image_features = model.encode_image(image_stack)\n",
    "        text_features = model.encode_text(text)\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        #print(\"Image features shape: \", image_features.shape, \"Text features shape: \", text_features.shape)\n",
    "\n",
    "        text_probs = torch.sigmoid(text_features @ image_features.T * model.logit_scale.exp() + model.logit_bias)\n",
    "\n",
    "    values, indices = torch.topk(text_probs, 3)\n",
    "\n",
    "    raw_image = images[indices[0][0].item()]\n",
    "    print(f\"{indices[0][0].item()}/{len(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0,200,\"['he is frustrated', 'for boy to play', 'hit drum', 'prevent boy from sntching it', 'shove it away']\",,why is the baby holding a drum in the beginning?,2,2735019707.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baby holding drum'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_siglip_query(\"why is the baby holding a drum in the beginning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = 2735019707\n",
    "question = \n",
    "options = \n",
    "\n",
    "utils.answer_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.get_data(args.query_file)\n",
    "ids, choices, queries, answers, vid_names = data\n",
    "for i in tqdm(range(len(ids))):\n",
    "    if i <= 2:\n",
    "        continue\n",
    "    video_id = vid_names[i].split(\".\")[0]\n",
    "    question = queries[i]\n",
    "    options = choices[i]\n",
    "    #video = utils.get_video(f'/shared/shang/datasets/nextqa/videos/{video_id}.mp4')\n",
    "    query = get_siglip_query(question)\n",
    "    print(video_id)\n",
    "    try:\n",
    "        output = utils.answer_question(video_id, question, options, query)\n",
    "        with open(f'./outputs/results_{args.query_file}', 'a') as f:\n",
    "            f.write(f'{ids[i]},{video_id},{question},\"{options}\",{output[0]},{answers[i]}\\n')\n",
    "    except:\n",
    "        with open(f'./outputs/failures_{args.query_file}', 'a') as f:\n",
    "            f.write(f'{ids[i]},{video_id},{question},\"{options}\",{output[0]},{answers[i]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vid_vqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
