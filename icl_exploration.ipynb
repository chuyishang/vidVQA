{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules\n",
    "import answerer\n",
    "from dataset import MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(data_path='/shared/shang/datasets/nextqa/videos/',\n",
       "          query_file='/shared/shang/datasets/nextqa/metadata/splits/val/val_queries_t.csv',\n",
       "          start_sample=0,\n",
       "          max_samples=2000,\n",
       "          gpu1=4,\n",
       "          gpu2=5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "d = {\n",
    "    \"data_path\": \"/shared/shang/datasets/nextqa/videos/\",\n",
    "    \"query_file\": \"/shared/shang/datasets/nextqa/metadata/splits/val/val_queries_t.csv\",\n",
    "    \"start_sample\": 0,\n",
    "    \"max_samples\": 2000,\n",
    "    \"gpu1\": 4,\n",
    "    \"gpu2\": 5,\n",
    "}\n",
    "\n",
    "args = SimpleNamespace(**d)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>possible_answers</th>\n",
       "      <th>query_type</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>video_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>['drink again', 'shake its body', 'smells the ...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the white dog do after going to the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2834146886.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>['put hand in mouth', 'continue skating', 'jum...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the female skater do after the male ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3441428429.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>['grab her', 'feed horse with grass', 'run tow...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the girl in white do after bending d...</td>\n",
       "      <td>1</td>\n",
       "      <td>6356067859.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>['look at him', 'touch their chests', 'kick hi...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the man do after the lady appear to ...</td>\n",
       "      <td>3</td>\n",
       "      <td>5296635780.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>['open her eyes', 'move toward the slides', 't...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the baby do after letting go of the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>6136926089.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>['sing', 'walk forward and observe', 'hit cans...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the man in white do after moving for...</td>\n",
       "      <td>2</td>\n",
       "      <td>6018490041.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>['smile and wants to pet it', 'backed away', '...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the dog do after the baby touches an...</td>\n",
       "      <td>1</td>\n",
       "      <td>7416295940.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>['hug the girl', 'smile', 'stand up', 'continu...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what did the girl do after she finished reciti...</td>\n",
       "      <td>2</td>\n",
       "      <td>3943634344.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>['mount the black dog', 'look around', 'looks ...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the brown dog do after stepping over...</td>\n",
       "      <td>1</td>\n",
       "      <td>4094488636.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>['move his legs', 'suck his thumb', 'raised hi...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what did the baby do after he approached near ...</td>\n",
       "      <td>2</td>\n",
       "      <td>3429509208.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  sample_id                                   possible_answers  \\\n",
       "0       0          3  ['drink again', 'shake its body', 'smells the ...   \n",
       "1       0          7  ['put hand in mouth', 'continue skating', 'jum...   \n",
       "2       0          8  ['grab her', 'feed horse with grass', 'run tow...   \n",
       "4       0         13  ['look at him', 'touch their chests', 'kick hi...   \n",
       "6       0         20  ['open her eyes', 'move toward the slides', 't...   \n",
       "7       0         23  ['sing', 'walk forward and observe', 'hit cans...   \n",
       "9       0         27  ['smile and wants to pet it', 'backed away', '...   \n",
       "10      0         29  ['hug the girl', 'smile', 'stand up', 'continu...   \n",
       "12      0         39  ['mount the black dog', 'look around', 'looks ...   \n",
       "13      0         45  ['move his legs', 'suck his thumb', 'raised hi...   \n",
       "\n",
       "   query_type                                              query  answer  \\\n",
       "0          TN  what does the white dog do after going to the ...       2   \n",
       "1          TN  what does the female skater do after the male ...       1   \n",
       "2          TN  what does the girl in white do after bending d...       1   \n",
       "4          TN  what does the man do after the lady appear to ...       3   \n",
       "6          TN  what does the baby do after letting go of the ...       1   \n",
       "7          TN  what does the man in white do after moving for...       2   \n",
       "9          TN  what does the dog do after the baby touches an...       1   \n",
       "10         TN  what did the girl do after she finished reciti...       2   \n",
       "12         TN  what does the brown dog do after stepping over...       1   \n",
       "13         TN  what did the baby do after he approached near ...       2   \n",
       "\n",
       "        video_name  \n",
       "0   2834146886.mp4  \n",
       "1   3441428429.mp4  \n",
       "2   6356067859.mp4  \n",
       "4   5296635780.mp4  \n",
       "6   6136926089.mp4  \n",
       "7   6018490041.mp4  \n",
       "9   7416295940.mp4  \n",
       "10  3943634344.mp4  \n",
       "12  4094488636.mp4  \n",
       "13  3429509208.mp4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "val_temporal = pd.read_csv('/shared/shang/datasets/nextqa/metadata/splits/val/val_queries_t.csv')\n",
    "val_temporal[val_temporal['query_type'] == 'TN'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(data_path=args.data_path,\n",
    "                    query_file=args.query_file,\n",
    "                    start_sample=args.start_sample,\n",
    "                    max_samples=args.max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cd6d90c1ad40bca056432291ec2b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "siglip = modules.SiglipModel(gpu_number=args.gpu1, siglip_model_type=\"ViT-B-16-SigLIP\")\n",
    "llava = modules.LLAVA(gpu_number=args.gpu2)\n",
    "llm = modules.GPTModel()\n",
    "ans = answerer.Answerer(llava, llava, siglip, llm)\n",
    "\n",
    "batch_correct = 0\n",
    "total_correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2834146886.mp4\n",
      "what does the white dog do after going to the cushion?\n",
      "['drink again', 'shake its body', 'smells the black dog', 'wagging tail', 'touch lady in blue stripes']\n",
      "correct answer 2\n",
      "right before evaluating ast in planner\n",
      "{\"Explanation\": \"The current information we have describes the action of the white dog going to the cushion. However, the question is about what the white dog does after going to the cushion. We need to identify the action of the white dog after going to the cushion in more detail. Since the current information does not provide this, we need to view more frames after the dog goes to the cushion to get a complete understanding.\", \"Plan\": [\"Move to the frames after the white dog is going to the cushion.\", \"Describe the action of the white dog in the frame after it goes to the cushion.\"]}\n",
      "{'Explanation': \"The plan requires us to find the frame after the white dog goes to the cushion and describe its action. Since we are currently at the 43.03333333333333 second mark in a 62.733333333333334 second video, we are already at a suitable frame and do not need to move elsewhere. We can directly ask for the action of the white dog in the current frame.\", 'Go-To': 43.03333333333333, 'Questions': ['What is the white dog doing after going to the cushion?']}\n",
      "right before evaluating ast in planner\n",
      "{\"Explanation\": \"The plan shows that we have already found the frame where the white dog is going to the cushion, and so this step is no longer found in the plan. Recall that our question is a temporal question. We need to find what the white dog is doing after going to the cushion. We will modify the plan to describe the action of the white dog in the frame after it goes to the cushion.\", \"Plan\": [\"Describe the action of the white dog in the frame after it goes to the cushion.\"]}\n",
      "{'Explanation': \"The question specifies a temporal action of the white dog after going to the cushion. Since we have a specific action to identify, we should focus on a frame after the white dog goes to the cushion. We are currently on the 43.03333333333333 second mark of a 62.733333333333334 second video, so moving forward by a second leads us to the 44.03333333333333 second mark. We can focus on this frame to understand the action of the white dog after going to the cushion. Thus, we can ask a direct question related to the action to get the necessary information.\", 'Go-To': 44.03333333333333, 'Questions': ['What is the white dog doing after going to the cushion?']}\n",
      "invalid syntax (<unknown>, line 1)\n",
      "right before evaluating ast in planner\n",
      "{\n",
      "  \"Explanation\": \"Based on the information provided, we have already found the relevant frames in which the white dog is interacting with the cushion. Since the question is about the action of the white dog after going to the cushion, we need to modify the plan to ask specific questions about the answer choices. We should inquire about each of the actions mentioned in the choice to accurately determine the action of the white dog after going to the cushion.\",\n",
      "  \"Plan\": [\n",
      "    \"Ask if the white dog is drinking again after going to the cushion.\",\n",
      "    \"Ask if the white dog is shaking its body after going to the cushion.\",\n",
      "    \"Ask if the white dog smells the black dog after going to the cushion.\",\n",
      "    \"Ask if the white dog is wagging its tail after going to the cushion.\",\n",
      "    \"Ask if the white dog is touching the lady in blue stripes after going to the cushion.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"Explanation\": \"The plan and the question both require us to identify the action of the white dog after going to the cushion. The current frame does not contain relevant information to answer the question, and the next frame we have that features the white dog doing something is at the 44.03333333333333/62.733333333333334 mark. We want to discern the action of the white dog after it goes to the cushion, so we need to look at the frames after. Thus, we would move forward in the video but looking at the frame right after the current frame, which is approximately 44.7 seconds. Then, we can ask if the white dog is exhibiting any of the provided actions to find out what it is doing after going to the cushion.\", \n",
      "  \"Go-To\": 44.7,\n",
      "  \"Questions\": [\"Is the white dog drinking again after going to the cushion?\", \"Is the white dog shaking its body after going to the cushion?\", \"Is the white dog smelling the black dog after going to the cushion?\", \"Is the white dog wagging its tail after going to the cushion?\", \"Is the white dog touching the lady in blue stripes after going to the cushion?\"]\n",
      "}\n",
      "right before evaluating ast in planner\n",
      "{\"Explanation\": \"The provided information includes multiple frames of the white dog engaging in different activities after going to the cushion. Since each frame provides a different action, we must revise our plan to ask specific questions related to each answer choice to determine the action taken by the white dog. This will allow us to gather the most accurate information related to the actions of the white dog after going to the cushion.\", \"Plan\": [\"Ask if the white dog is drinking again after going to the cushion.\", \"Ask if the white dog is shaking its body after going to the cushion.\", \"Ask if the white dog smells the black dog after going to the cushion.\", \"Ask if the white dog is wagging its tail after going to the cushion.\", \"Ask if the white dog is touching the lady in blue stripes after going to the cushion.\"]}\n",
      "{'Explanation': 'The plan requires us to ask about the specific actions of the white dog after going to the cushion. We have already retrieved a relevant answer in the frame at 44.7 seconds. Since our goal is to provide an accurate answer, we do not need to move to another frame. Instead, we can ask more specific questions to clarify the actions described in the previous answers. Therefore, we should stay at the frame at 44.7 seconds and ask targeted questions to verify the previously described actions of the white dog.', 'Go-To': 44.7, 'Questions': ['Is the white dog drinking again after going to the cushion?', 'Is the white dog shaking its body after going to the cushion?', 'Is the white dog smelling the black dog after going to the cushion?', 'Is the white dog wagging its tail after going to the cushion?', 'Is the white dog touching the lady in blue stripes after going to the cushion?']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item0 = dataset[0]\n",
    "video0 = dataset.construct_video(item0)\n",
    "print(item0['video_name'])\n",
    "print(item0['query'])\n",
    "print(item0['possible_answers'])\n",
    "print('correct answer', item0['answer'])\n",
    "\n",
    "pred0 = ans.forward(video0)\n",
    "pred0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3441428429.mp4\n",
      "what does the female skater do after the male skater puts her back down on the ice?\n",
      "['put hand in mouth', 'continue skating', 'jump', 'move her arms up and down', 'laugh and run forward']\n",
      "correct answer: 1\n",
      "right before evaluating ast in planner\n",
      "{\"Explanation\": \"The current description does not have any specific time references, so it is unclear whether the male skater has already put the female skater back down on the ice. First, we need to identify the frame where the male skater puts the female skater back down on the ice. Then, we can describe the action of the female skater in that frame to understand what she does after being put back down on the ice.\", \"Plan\": [\"We need to find the frame where the male skater puts the female skater back down on the ice.\", \"We need to describe the action of the female skater in the frame after she is put back down on the ice.\"]}\n",
      "{\n",
      "  \"Explanation\": \"Finding the frame where the male skater puts the female skater back down on the ice is the first step in the plan. The question asks about the action of the female skater in the frame after she is put back down on the ice. Since the current frame is close to the end of the video at 89.97 seconds, we should move to a frame near this time to observe the action of the female skater after being placed down by the male skater. Let's choose to go to the 85th second, then we should ask about the action of the female skater in this frame.\", \n",
      "  \"Go-To\": 85.0, \n",
      "  \"Questions\": [\"What is the female skater doing after being put back down on the ice?\"]\n",
      "}\n",
      "right before evaluating ast in planner\n",
      "{\"Explanation\": \"Our current plan already includes steps to find the frame when the male skater puts the female skater back down on the ice and to describe the action of the female skater in the frame after she is put back down on the ice. However, we need additional information about the female skater's action after being put back on the ice in order to answer the question. Since we have some information about her action after being put back on the ice, we can add a step to ask further questions about her actions in additional frames.\", \"Plan\": [\"Move to the frames after the female skater is put back down on the ice.\", \"Ask about the specific action of the female skater in each of these frames.\"]}\n",
      "{'Explanation': \"The plan requires us to move to the frames after the female skater is put back down on the ice and ask about the specific action of the female skater in each of these frames. We can follow the plan by moving 2 seconds forward, as we are currently at the 85th second in a 89.97 second video, making the timestamp of the next frame 87 seconds. Then, we should ask about the specific action of the female skater in the frame at 87 seconds to fulfill the plan.\", 'Go-To': 87.0, 'Questions': [\"What is the female skater doing after being put back down on the ice?\"]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct\n",
    "item1 = dataset[1]\n",
    "video1 = dataset.construct_video(item1)\n",
    "print(item1['video_name'])\n",
    "print(item1['query'])\n",
    "print(item1['possible_answers'])\n",
    "print('correct answer:', item1['answer'])\n",
    "\n",
    "pred1 = ans.forward(video1)\n",
    "pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6356067859.mp4\n",
      "what does the girl in white do after bending down in the middle?\n",
      "['grab her', 'feed horse with grass', 'run towards the camera', 'umbrella', 'put her arms up']\n",
      "correct answer: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct\n",
    "item2 = dataset[2]\n",
    "video2 = dataset.construct_video(item2)\n",
    "print(item2['video_name'])\n",
    "print(item2['query'])\n",
    "print(item2['possible_answers'])\n",
    "print('correct answer:', item2['answer'])\n",
    "\n",
    "pred2 = ans.forward(video2)\n",
    "pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5296635780.mp4\n",
      "what does the man do after the lady appear to punch him and smiles?\n",
      "['look at him', 'touch their chests', 'kick him', 'lean forward', 'sit back properly']\n",
      "correct answer: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incorrect\n",
    "# predicts None insetad of 3\n",
    "item3 = dataset[4]\n",
    "video3 = dataset.construct_video(item3)\n",
    "print(item3['video_name'])\n",
    "print(item3['query'])\n",
    "print(item3['possible_answers'])\n",
    "print('correct answer:', item3['answer'])\n",
    "\n",
    "pred3 = ans.forward(video3)\n",
    "pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6136926089.mp4\n",
      "what does the baby do after letting go of the cart?\n",
      "['open her eyes', 'move toward the slides', 'touch the toy', 'turn to his back', 'lean forward and put head down']\n",
      "correct answer: 1\n",
      "right before evaluating ast in planner\n",
      "{\"Explanation\": \"The current information does not provide a frame where the baby is letting go of the cart. Given that, we need to take actions in a sequential manner to find the frame where the baby lets go of the cart and then describe the scene around the baby in detail to collect more information to answer the question.\", \"Plan\": [\"We need to move forward in the video to identify the frames where the baby is letting go of the cart.\", \"We need to describe the scene around the baby in detail.\"]}\n",
      "{'Explanation': \"The plan states that we need to move forward in the video to identify the frames where the baby is letting go of the cart. As we are currently at the 8.67 second mark in a 50.77 second video, we need to move forward to find the frame where the baby is letting go of the cart. Since we need to keep track of where we are in the video to effectively find the right frame to move to, the simplest question to ask to fulfill the plan is 'Is the baby letting go of the cart?'. We can move forward by performing frame splitting. Since there are no other frames in INFO, we can move to the frame in between the current frame and the end, which is (8.67 + 50.77) / 2 = 29.72 seconds. Thus, we should go to the 29.72 second mark.\", 'Go-To': 29.72, 'Questions': ['Is the baby letting go of the cart?']}\n",
      "right before evaluating ast in planner\n",
      "{\"Explanation\": \"The plan shows that we have already found the frame where the baby is letting go of the cart, and so this step is no longer needed in the plan. The question asks about an action after the baby lets go of the cart, so we need to move to a frame after this event. Then, we need to describe the scene around the baby in detail to collect more information. Since our past plan already has the step to describe the scene around the baby, we can continue using these steps.\", \"Plan\": [\"We need to move to the frames after the baby is letting go of the cart.\", \"We need to describe the scene around the baby in detail.\"]}\n",
      "{'Explanation': \"We have successfully found the frame where the baby is letting go of the cart, and now we need to move to the frames after this action. The timestamp of the current frame is 29.72 seconds out of a 50.766666666666666 second video. We need to look at the frames post this action to understand what the baby does after letting go of the cart. To do this, we can move to a frame slightly after this timestamp, which would be (29.72 seconds + 50.766666666666666 seconds) / 2 = 40.24333333333333 seconds. We could ask questions such as 'What is the baby doing after letting go of the cart?' to obtain further information.\",\n",
      " 'Go-To': 40.24333333333333,\n",
      " 'Questions': ['Is there a baby? What is the baby doing after letting go of the cart?']}\n",
      "right before evaluating ast in planner\n",
      "{\n",
      "  \"Explanation\": \"The question asks about the action of the baby after letting go of the cart. From the information provided, it seems that the action after letting go of the cart is not clearly described. Therefore, to accurately answer the question, we need to identify the frames after the baby lets go of the cart and gather detailed descriptions of the scene around the baby to understand the subsequent action.\",\n",
      "  \"Plan\": [\n",
      "    \"We need to view the frames following the frame where the baby is letting go of the cart.\",\n",
      "    \"We need to describe the scene around the baby in detail to get more information.\"\n",
      "  ]\n",
      "}\n",
      "{'Explanation': \"From the plan, we have already found the frame where the baby is letting go of the cart, but there is no additional information in this frame that helps us answer the question. Our question is a temporal question, and we need to look at the frames after this frame. Since the previous frame (time 29.72s) already contains some information about the baby letting go of the cart, we should try moving forward in smaller time increments to identify the frame where the baby is seen after letting go of the cart. We can move forward in smaller increments, such as 1 second, from the frame at 40.24333333333333s to explore the details near the action and ask more questions about the baby's actions.\", 'Go-To': 41.24333333333333, 'Questions': ['Is there a baby?', 'What is the baby doing now?']}\n",
      "right before evaluating ast in planner\n",
      "{\n",
      "  \"Explanation\": \"Based on the given info, we have found the frame where the baby let go of the cart. Our objective is to identify the action the baby performs after letting go of the cart. Since we don't have specific information on the immediate action following the baby letting go of the cart, we need to modify our plan to look for additional frames following the event to find out the action and gather more details about the scene.\",\n",
      "  \"Plan\": [\n",
      "    \"Move to the frames after the baby is letting go of the cart.\",\n",
      "    \"Describe the scene around the baby in detail to collect more information.\"\n",
      "  ]\n",
      "}\n",
      "{'Explanation': \"According to the plan, we have already located the frame where the baby is letting go of the cart, so this step is no longer necessary. The question asks about the baby's action after letting go of the cart, so we need to view the frames after this action. The next available frame after the current frame is at around the 40 second mark, so to move forward as per the plan, we will go to 40.5 seconds. After moving to this frame, since our current frame has irrelevant details, we should ask a direct question to fulfill the plan. Thus, the simplest question to fulfill the plan would be 'What is the baby doing now?'\",\n",
      " 'Go-To': 40.5,\n",
      " 'Questions': [\"Is there a baby? What is the baby doing now?\"]}\n",
      "right before evaluating ast in planner\n",
      "{\n",
      "  \"Explanation\": \"From the information given, we have already found the frame where the baby is letting go of the cart, and so this step is no longer found in the plan. Our question is a temporal question. We need to find what the baby is doing after letting go of the cart. We will modify the plan to traverse the frames after the frame where the baby is letting go of the cart, and remove the first step.\",\n",
      "  \"Plan\": [\"Move to the frames after the baby is letting go of the cart.\", \"Describe the scene around the baby in detail to collect more information.\"]\n",
      "}\n",
      "{\n",
      "  \"Explanation\": \"The plan shows that we need to find the frames after the baby lets go of the cart and describe the scene around the baby in detail to gather more information. We are currently at the 40.5 second mark out of 50.766666666666666 seconds, and there is no relevant information in this frame. We need to move to the frames after the baby lets go of the cart. Since the next frame after 40.5 seconds has no relevant information about the baby, we can move to the frame after that by performing frame splitting. The frame between the current frame and the end would be (40.5 + 50.766666666666666) / 2 = 45.63333333333333 seconds. Thus, we should go to the 45.63333333333333 second mark.\",\n",
      "  \"Go-To\": 45.63333333333333,\n",
      "  \"Questions\": [\"Is there a baby?\", \"What is the baby doing now?\"]\n",
      "}\n",
      "right before evaluating ast in planner\n",
      "{\"Explanation\": \"We have multiple frames where the baby acts after letting go of the cart. However, the previous plan only involves describing the scene which may not necessarily help to discern the specific action of the baby in question. We need to focus on identifying the specific action the baby takes after letting go of the cart to decipher the answer.\", \"Plan\": [\"Carefully observe each frame after the baby lets go of the cart to identify the specific action taken by the baby.\", \"We need to ask about each action choice from the question to get a clearer understanding of the baby's action.\"]}\n",
      "{'Explanation': \"The plan requires carefully observing each frame after the baby lets go of the cart to identify the specific action taken by the baby. We are currently at the time 45.63333333333333 seconds out of 50.766666666666666, which means we can move forward by performing frame splitting. The frame between the current frame and the end is (45.63333333333333 + 50.766666666666666) / 2 = 48.2 seconds. Thus, we should go to the 48.2 second mark. Then, we will ask about each action choice from the question to get a clearer understanding of the baby's action.\", 'Go-To': 48.2, 'Questions': [\"Is there a baby\", \"What is the baby doing now?\", \"Did the baby open her eyes?\", \"Did the baby move toward the slides?\", \"Did the baby touch the toy?\", \"Did the baby turn to his back?\", \"Did the baby lean forward and put her head down?\"]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incorrect\n",
    "# predicts 2 instead of 1\n",
    "item4 = dataset[6]\n",
    "video4 = dataset.construct_video(item4)\n",
    "print(item4['video_name'])\n",
    "print(item4['query'])\n",
    "print(item4['possible_answers'])\n",
    "print('correct answer:', item4['answer'])\n",
    "\n",
    "pred4 = ans.forward(video4)\n",
    "pred4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6018490041.mp4\n",
      "what does the man in white do after moving forward at the start?\n",
      "['sing', 'walk forward and observe', 'hit cans', 'clicks the picture', 'start dancing']\n",
      "correct answer: 2\n",
      "right before evaluating ast in planner\n",
      "{\"Explanation\": \"We are at the start of the video, and the information provided does not align with the question. We need to go forward in the video to find the frame where the man in white moves forward. Then, since the question asks about an action after moving forward, we need to describe the scene in detail after we locate this action to provide us information to help us understand why.\", \"Plan\": [\"We need to move forwards in the video to identify the frames where the man in white moves forward.\", \"We need to see what action the man in white takes after moving forward.\"]}\n",
      "{'Explanation': \"Since the current timestamp is at the start of the video, we need to go forwards to identify the frames where the man in white moves forward. To do this, we can go to around the middle of the video to look for the man moving forward. The middle of 27.5 seconds is 27.5/2 = 13.75 seconds. Thus, we should go to the 13.75 second mark.\",\n",
      " 'Go-To': 13.75,\n",
      " 'Questions': [\"Is there a man in white?\",\n",
      "               \"What is the man in white doing after moving forward?\"]}\n",
      "right before evaluating ast in planner\n",
      "{\"Explanation\": \"We have located the frame where the man in white moves forward and his action after, which is he is bending down to pick up a small child. This means that we have completed the first two steps of the plan. Since the question asks about a specific action, we can stop at this point and provide our answer based on the info we have gathered.\", \"Plan\": [\"N/A\", \"N/A\"]}\n",
      "{'Explanation': 'We need to see after the man moves forward at the start, which suggests we should go near the start of the video. We are currently at the 13.75 second mark, so we should move backwards by around half that time, which would be roughly 6.875 seconds. This will take us near the beginning of the video to observe the man in white moving forward. In this frame, we should ask if there is a man in white and if he is moving forward to make sure we have the correct frame.', 'Go-To': 6.875, 'Questions': ['Is there a man in white?', 'Is the man in white moving forward?']}\n",
      "right before evaluating ast in planner\n",
      "{\n",
      "  \"Explanation\": \"This question is asking about the action of the man in white after moving forward at the start. We can accomplish that by identifying the frames in which the man in white is moving forward and the following action. We then need to describe the scene around the man in white in detail to collect additional information. Given the information we have, we need to identify the man in white's movement forward, then moving forward to the following actions and then describing the scene in detail.\",\n",
      "  \"Plan\": [\n",
      "    \"Identify the frames where the man in white is moving forward.\",\n",
      "    \"Move to the frames after the man in white moving forward to identify the following actions.\",\n",
      "    \"Describe the scene around the man in white in detail.\"\n",
      "  ]\n",
      "}\n",
      "{'Explanation': \"We have found the frame where the man in white is moving forward, and we need to find the frames after this moment. The current timestamp is 6.875 seconds out of 27.5 seconds of the video. As the next step after moving forward is to find subsequent actions, we need to find a frame after this one. We can select a time that is midway between the current time and the end of the video to potentially observe the next action. Hence, we should go to the 20-second mark. By moving to this frame, we can ask if the man in white is performing an action after moving forward to fulfill the plan.\", 'Go-To': 20.0, 'Questions': ['Is there a man in white?', 'What is the man in white doing after moving forward?']}\n",
      "right before evaluating ast in planner\n",
      "{\"Explanation\": \"We need to identify the action of the man in white after moving forward. Based on our information, we are given descriptions of multiple scenes where the man in white is moving forward. The descriptions include a variety of activities taking place at different times. Given the temporal nature of the question, we should restrict our analysis to the frames that occur right after the man in white moves forward. To better understand the specific action the man takes after moving forward, we may need to move to the following frames and focus our observations on his subsequent actions. Hence, we will modify the plan to focus on the frames after the man in white moves forward, removing the initial step.\", \"Plan\": [\"Move to the frames after the man in white moving forward to identify the following actions.\", \"Describe the scene around the man in white in detail.\"]}\n",
      "{\n",
      "  'Explanation': \"We have already found the image where the man in white is after moving forward. There is no additional information in this frame that helps us answer the question. The plan indicates that the next step is to describe the scene around the man in white in detail. Thus, to fulfill the remaining steps in the plan, we can ask the simplest question about the immediate surroundings, such as 'What is happening in the surroundings?' The current timestamp is 20.0 out of 27.5 seconds. To view the surroundings, it's most effective to go to the end of the video, so we choose to go to the 27th second since the additional details about the man's surroundings will be available there.\",\n",
      "  'Go-To': 27.0,\n",
      "  'Questions': [\"What is happening in the surroundings?\"]\n",
      "}\n",
      "right before evaluating ast in planner\n",
      "{\n",
      "  \"Explanation\": \"The previous plan is focused on identifying the actions of the man in white after moving forward. The descriptions of the frames provide more context around the man's actions. However, there are multiple descriptions of the scene, and only one of them matches the action of the man in white moving forward. Thus, we need to focus on identifying and selecting the frame that specifically captures the action of the man in white moving forward, and then describe the scene around that specific frame in detail to obtain the relevant information to answer the question.\",\n",
      "  \"Plan\": [\"Identify the frame specifically showing the man in white moving forward.\", \"Describe the scene around the man in white in detail.\"]\n",
      "}\n",
      "{\n",
      "  \"Explanation\": \"The plan indicates that we need to identify the frame specifically showing the man in white moving forward. We know that the video is 27.5 seconds long, and the current frame is at the end of the video. To fulfill the plan, we should move to a frame at the start of the video, where we can see the man in white moving forward. Since we need to observe what the man in white does after moving forward, a valid approach would be to move to the 0.5 second mark and observe the actions of the man in white. This will provide us with the necessary information to describe the actions and the scene around the man in white.\", \n",
      "  \"Go-To\": 0.5, \n",
      "  \"Questions\": [\"Is there a man in white?\", \"What is the man in white doing?\"]\n",
      "}\n",
      "right before evaluating ast in planner\n",
      "{\"Explanation\": \"Our question asks about the actions of a man in white after moving forward at the start. Our past plan is designed to identify the frame specifically showing the man in white moving forward. The current info contains multiple instances of the man in white, each showing different actions. We need to update our plan to ensure we capture all instances after the man in white moves forward and describe the scenes to gather detailed information.\", \"Plan\": [\"Identify the frames showing the man in white moving forward.\", \"Move to the frames after the man in white is moving forward.\", \"Describe the scenes around the man in white in detail.\"]}\n",
      "{'Explanation': \"The plan asks us to find the frames where the man in white is moving forward at the start to identify what he does after. We have information about a few frames after the start but don't have a frame specifically after the man in white moves forward. We currently are at the 0.5s mark. We need to move to the frames after the man in white moves forward, but we don't have that specific information. Therefore we should explore the frames by moving forward by performing frame splitting. Since there are no other frames, we can move to the frame in between the current frame and the end, which is (0.5 + 27.5) / 2 = 14.0 seconds. Thus, we should go to the 14.0 second mark.\", 'Go-To': 14.0, 'Questions': ['Is there a man in white? What is the man in white doing after moving forward?']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incorrect\n",
    "# predicts 3 instead of 2\n",
    "item5 = dataset[7]\n",
    "video5 = dataset.construct_video(item5)\n",
    "print(item5['video_name'])\n",
    "print(item5['query'])\n",
    "print(item5['possible_answers'])\n",
    "print('correct answer:', item5['answer'])\n",
    "\n",
    "pred5 = ans.forward(video5)\n",
    "pred5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item6 = dataset[9]\n",
    "video6 = dataset.construct_video(item6)\n",
    "print(item6['video_name'])\n",
    "print(item6['query'])\n",
    "print(item6['possible_answers'])\n",
    "print('correct answer:', item6['answer'])\n",
    "\n",
    "pred6 = ans.forward(video6)\n",
    "pred6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(dataset))):\n",
    "    item = dataset[i]\n",
    "    video = dataset.construct_video(item)\n",
    "    try:\n",
    "        pred = ans.forward(video)\n",
    "        with open(args.output_file, \"a\") as outfile:\n",
    "            outfile.write(f\"{item['index']},{item['video_name']},{item['query_type']},{item['query']},{item['answer']},{item['possible_answers']},{pred}\\n\")\n",
    "        if pred == item[\"answer\"]:\n",
    "            print(\"correct\")\n",
    "            batch_correct += 1\n",
    "            total_correct += 1\n",
    "        if i+1 % args.print_interval == 0:\n",
    "            print(\"Batch accuracy: \", batch_correct / args.print_interval)\n",
    "            batch_correct = 0\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videovqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15becb0ea3c5f09f0798b2d6295e5f1f2749f31b0fc205df36d65e3a7a669c71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
