{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules\n",
    "import answerer\n",
    "from dataset import MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(data_path='/shared/shang/datasets/nextqa/videos/',\n",
       "          query_file='/shared/shang/datasets/nextqa/metadata/splits/val/val_queries_t.csv',\n",
       "          start_sample=0,\n",
       "          max_samples=2000,\n",
       "          gpu1=4,\n",
       "          gpu2=5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "d = {\n",
    "    \"data_path\": \"/shared/shang/datasets/nextqa/videos/\",\n",
    "    \"query_file\": \"/shared/shang/datasets/nextqa/metadata/splits/val/val_queries_t.csv\",\n",
    "    \"start_sample\": 0,\n",
    "    \"max_samples\": 2000,\n",
    "    \"gpu1\": 4,\n",
    "    \"gpu2\": 5,\n",
    "}\n",
    "\n",
    "args = SimpleNamespace(**d)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>possible_answers</th>\n",
       "      <th>query_type</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>video_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>['drink again', 'shake its body', 'smells the ...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the white dog do after going to the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2834146886.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>['put hand in mouth', 'continue skating', 'jum...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the female skater do after the male ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3441428429.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>['grab her', 'feed horse with grass', 'run tow...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the girl in white do after bending d...</td>\n",
       "      <td>1</td>\n",
       "      <td>6356067859.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>['look at him', 'touch their chests', 'kick hi...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the man do after the lady appear to ...</td>\n",
       "      <td>3</td>\n",
       "      <td>5296635780.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>['open her eyes', 'move toward the slides', 't...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the baby do after letting go of the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>6136926089.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>['sing', 'walk forward and observe', 'hit cans...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the man in white do after moving for...</td>\n",
       "      <td>2</td>\n",
       "      <td>6018490041.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>['smile and wants to pet it', 'backed away', '...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the dog do after the baby touches an...</td>\n",
       "      <td>1</td>\n",
       "      <td>7416295940.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>['hug the girl', 'smile', 'stand up', 'continu...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what did the girl do after she finished reciti...</td>\n",
       "      <td>2</td>\n",
       "      <td>3943634344.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>['mount the black dog', 'look around', 'looks ...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what does the brown dog do after stepping over...</td>\n",
       "      <td>1</td>\n",
       "      <td>4094488636.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>['move his legs', 'suck his thumb', 'raised hi...</td>\n",
       "      <td>TN</td>\n",
       "      <td>what did the baby do after he approached near ...</td>\n",
       "      <td>2</td>\n",
       "      <td>3429509208.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  sample_id                                   possible_answers  \\\n",
       "0       0          3  ['drink again', 'shake its body', 'smells the ...   \n",
       "1       0          7  ['put hand in mouth', 'continue skating', 'jum...   \n",
       "2       0          8  ['grab her', 'feed horse with grass', 'run tow...   \n",
       "4       0         13  ['look at him', 'touch their chests', 'kick hi...   \n",
       "6       0         20  ['open her eyes', 'move toward the slides', 't...   \n",
       "7       0         23  ['sing', 'walk forward and observe', 'hit cans...   \n",
       "9       0         27  ['smile and wants to pet it', 'backed away', '...   \n",
       "10      0         29  ['hug the girl', 'smile', 'stand up', 'continu...   \n",
       "12      0         39  ['mount the black dog', 'look around', 'looks ...   \n",
       "13      0         45  ['move his legs', 'suck his thumb', 'raised hi...   \n",
       "\n",
       "   query_type                                              query  answer  \\\n",
       "0          TN  what does the white dog do after going to the ...       2   \n",
       "1          TN  what does the female skater do after the male ...       1   \n",
       "2          TN  what does the girl in white do after bending d...       1   \n",
       "4          TN  what does the man do after the lady appear to ...       3   \n",
       "6          TN  what does the baby do after letting go of the ...       1   \n",
       "7          TN  what does the man in white do after moving for...       2   \n",
       "9          TN  what does the dog do after the baby touches an...       1   \n",
       "10         TN  what did the girl do after she finished reciti...       2   \n",
       "12         TN  what does the brown dog do after stepping over...       1   \n",
       "13         TN  what did the baby do after he approached near ...       2   \n",
       "\n",
       "        video_name  \n",
       "0   2834146886.mp4  \n",
       "1   3441428429.mp4  \n",
       "2   6356067859.mp4  \n",
       "4   5296635780.mp4  \n",
       "6   6136926089.mp4  \n",
       "7   6018490041.mp4  \n",
       "9   7416295940.mp4  \n",
       "10  3943634344.mp4  \n",
       "12  4094488636.mp4  \n",
       "13  3429509208.mp4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "val_temporal = pd.read_csv('/shared/shang/datasets/nextqa/metadata/splits/val/val_queries_t.csv')\n",
    "val_temporal[val_temporal['query_type'] == 'TN'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(data_path=args.data_path,\n",
    "                    query_file=args.query_file,\n",
    "                    start_sample=args.start_sample,\n",
    "                    max_samples=args.max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2af1b5b8b9b49eb96aa383f04863962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "siglip = modules.SiglipModel(gpu_number=args.gpu1, siglip_model_type=\"ViT-B-16-SigLIP\")\n",
    "llava = modules.LLAVA(gpu_number=args.gpu2)\n",
    "llm = modules.GPTModel()\n",
    "ans = answerer.Answerer(llava, llava, siglip, llm)\n",
    "\n",
    "batch_correct = 0\n",
    "total_correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2834146886.mp4\n",
      "what does the white dog do after going to the cushion?\n",
      "['drink again', 'shake its body', 'smells the black dog', 'wagging tail', 'touch lady in blue stripes']\n",
      "correct answer 2\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/videovqa/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3550\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[6], line 8\u001b[0m\n    pred0 = ans.forward(video0)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/vidVQA/answerer.py:103\u001b[0m in \u001b[1;35mforward\u001b[0m\n    plan, explanation = self.planner.forward(self.video)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/vidVQA/answerer.py:215\u001b[0m in \u001b[1;35mforward\u001b[0m\n    output, explanation = self.create_plan(video)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/vidVQA/answerer.py:205\u001b[0m in \u001b[1;35mcreate_plan\u001b[0m\n    output = ast.literal_eval(output)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/videovqa/lib/python3.9/ast.py:62\u001b[0m in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string, mode='eval')\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/miniconda3/envs/videovqa/lib/python3.9/ast.py:50\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ```python\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "item0 = dataset[0]\n",
    "video0 = dataset.construct_video(item0)\n",
    "print(item0['video_name'])\n",
    "print(item0['query'])\n",
    "print(item0['possible_answers'])\n",
    "print('correct answer', item0['answer'])\n",
    "\n",
    "pred0 = ans.forward(video0)\n",
    "pred0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3441428429.mp4\n",
      "what does the female skater do after the male skater puts her back down on the ice?\n",
      "['put hand in mouth', 'continue skating', 'jump', 'move her arms up and down', 'laugh and run forward']\n",
      "correct answer: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct\n",
    "item1 = dataset[1]\n",
    "video1 = dataset.construct_video(item1)\n",
    "print(item1['video_name'])\n",
    "print(item1['query'])\n",
    "print(item1['possible_answers'])\n",
    "print('correct answer:', item1['answer'])\n",
    "\n",
    "pred1 = ans.forward(video1)\n",
    "pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6356067859.mp4\n",
      "what does the girl in white do after bending down in the middle?\n",
      "['grab her', 'feed horse with grass', 'run towards the camera', 'umbrella', 'put her arms up']\n",
      "correct answer: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct\n",
    "item2 = dataset[2]\n",
    "video2 = dataset.construct_video(item2)\n",
    "print(item2['video_name'])\n",
    "print(item2['query'])\n",
    "print(item2['possible_answers'])\n",
    "print('correct answer:', item2['answer'])\n",
    "\n",
    "pred2 = ans.forward(video2)\n",
    "pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5296635780.mp4\n",
      "what does the man do after the lady appear to punch him and smiles?\n",
      "['look at him', 'touch their chests', 'kick him', 'lean forward', 'sit back properly']\n",
      "correct answer: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incorrect\n",
    "# predicts None insetad of 3\n",
    "item3 = dataset[4]\n",
    "video3 = dataset.construct_video(item3)\n",
    "print(item3['video_name'])\n",
    "print(item3['query'])\n",
    "print(item3['possible_answers'])\n",
    "print('correct answer:', item3['answer'])\n",
    "\n",
    "pred3 = ans.forward(video3)\n",
    "pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6136926089.mp4\n",
      "what does the baby do after letting go of the cart?\n",
      "['open her eyes', 'move toward the slides', 'touch the toy', 'turn to his back', 'lean forward and put head down']\n",
      "correct answer: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incorrect\n",
    "# predicts 2 instead of 1\n",
    "item4 = dataset[6]\n",
    "video4 = dataset.construct_video(item4)\n",
    "print(item4['video_name'])\n",
    "print(item4['query'])\n",
    "print(item4['possible_answers'])\n",
    "print('correct answer:', item4['answer'])\n",
    "\n",
    "pred4 = ans.forward(video4)\n",
    "pred4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6018490041.mp4\n",
      "what does the man in white do after moving forward at the start?\n",
      "['sing', 'walk forward and observe', 'hit cans', 'clicks the picture', 'start dancing']\n",
      "correct answer: 2\n",
      "{'Explanation': \"This question asks about an action that occurs at the start, so we need to start from the beginning of the video. We are currently on the frame at 0.06666666666666667 seconds in a 27.5 second video, so we are nearly at the start. Since we are close to the start, we do not need to travel to another frame. To understand the action of the man in white after moving forward, we need to ask a simple question about the man's action after moving forward to fulfill the plan.\", 'Go-To': 0.06666666666666667, 'Questions': ['What is the man in white doing after moving forward?']}\n",
      "{'Explanation': \"Based on the plan and the question, we need to first identify the frame where the man in white moves forward. Since the video is 27.5 seconds long, we can choose to go to a frame around the middle of the video, near the 13.75 second mark, to see if the man in white performs the action described in the question. Therefore, we should go to the 13.75 second mark. It is estimated as half of the total duration.\", 'Go-To': 13.75, 'Questions': ['What is the man in white doing?', 'Describe the action of the man in white at this time?']}\n",
      "{'Explanation': 'The question asks about an action at the start, so we need to go back to the beginning of the video. We are currently at the 13.75 second mark in a 27.5 second video, so we need to go to a time that is close to the start. We choose to go back to the 5 second mark, which is roughly halfway backwards. Then, we should ask about what the man in white is doing after moving forward. We select this frame and ask a question such as \"What is the man in white doing after he moves forward at the start?\"',\n",
      " 'Go-To': 5.0,\n",
      " 'Questions': ['Is there a man wearing white?', 'What is the man in white doing after moving forward?']}\n",
      "{\n",
      "  'Explanation': \"According to the question, we have to find the man in white and observe his actions after moving forward. We are currently at the 5 second mark in a 27.5 second video. The plan requires us to observe the frames around the time when the man in white moves forward. So, to fulfill this, we should look at the frame after this one. We can move forwards by performing frame splitting. Since there are no other frames in the info, we can move to the frame in between the current frame and the end, which is (27.5 - 5.0) / 2 = 16.25 seconds. Thus, we should go to the 16.25 second mark.\",\n",
      "  'Go-To': 16.25,\n",
      "  'Questions': [\"Is there a man wearing white?\", \"What is the man in white doing after moving forward?\"]\n",
      "}\n",
      "{\n",
      "  \"Explanation\": \"The question asks about what the man in white does after moving forward at the start, so we need to find the frame where the man in white moves forward. We are currently at the 16.25 second mark in a 27.5 second video. To ensure we are close to the start, we should go to an earlier timestamp, roughly a quarter of the way through the video. We can choose to go to the 6.875 second mark which is around 25% of 27.5 seconds. Then, we should ask if there is a man in white, however, since we have more than one frame, we can refine by asking, 'Is there a man wearing white holding a cane?'.\",\n",
      "  \"Go-To\": 6.875,\n",
      "  \"Questions\": [\"Is there a man wearing white?\", \"Is there a man wearing white holding a cane?\"]\n",
      "}\n",
      "{\n",
      "  \"Explanation\": \"The question asks about what the man in white does after moving forward. Since the question asks about an action after moving forward, we need to move to the frames after the current frame. Since the current frame is 6.875 seconds out of 27.5, we can perform frame splitting to move to a frame after. We can move to the 6.875 + (27.5-6.875)/2 = 17.1875 seconds, which is the middle point between the current frame and the end. Then, to understand what the man in white does at this time, we should ask if the man in white is doing each of the answer choices after moving forward.\", \n",
      "  \"Go-To\": 17.1875, \n",
      "  \"Questions\": [\n",
      "    \"What is the man in white doing at 17.1875 seconds?\", \n",
      "    \"Is the man in white singing?\", \n",
      "    \"Is the man in white walking forward and observing?\", \n",
      "    \"Is the man in white hitting cans?\", \n",
      "    \"Is the man in white clicking the picture?\", \n",
      "    \"Is the man in white starting dancing?\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incorrect\n",
    "# predicts 3 instead of 2\n",
    "item5 = dataset[7]\n",
    "video5 = dataset.construct_video(item5)\n",
    "print(item5['video_name'])\n",
    "print(item5['query'])\n",
    "print(item5['possible_answers'])\n",
    "print('correct answer:', item5['answer'])\n",
    "\n",
    "pred5 = ans.forward(video5)\n",
    "pred5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item6 = dataset[9]\n",
    "video6 = dataset.construct_video(item6)\n",
    "print(item6['video_name'])\n",
    "print(item6['query'])\n",
    "print(item6['possible_answers'])\n",
    "print('correct answer:', item6['answer'])\n",
    "\n",
    "pred6 = ans.forward(video6)\n",
    "pred6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(dataset))):\n",
    "    item = dataset[i]\n",
    "    video = dataset.construct_video(item)\n",
    "    try:\n",
    "        pred = ans.forward(video)\n",
    "        with open(args.output_file, \"a\") as outfile:\n",
    "            outfile.write(f\"{item['index']},{item['video_name']},{item['query_type']},{item['query']},{item['answer']},{item['possible_answers']},{pred}\\n\")\n",
    "        if pred == item[\"answer\"]:\n",
    "            print(\"correct\")\n",
    "            batch_correct += 1\n",
    "            total_correct += 1\n",
    "        if i+1 % args.print_interval == 0:\n",
    "            print(\"Batch accuracy: \", batch_correct / args.print_interval)\n",
    "            batch_correct = 0\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videovqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15becb0ea3c5f09f0798b2d6295e5f1f2749f31b0fc205df36d65e3a7a669c71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
